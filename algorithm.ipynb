{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and classes\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM \n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import textwrap\n",
    "\n",
    "# Open the PDF file\n",
    "with open(\"./ipc-data.pdf\", \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    document = reader.pages[0].extract_text()\n",
    "\n",
    "# Initialize a text splitter to split documents into smaller chunks\n",
    "chunk_size = 500\n",
    "texts = textwrap.wrap(document, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "# Create a list of Document objects\n",
    "documents = [Document(text) for text in texts]\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")\n",
    "# Create a Chroma vector database from the documents\n",
    "db = Chroma.from_documents(documents, embeddings, persist_directory='./persist_directory/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the checkpoint for the language model\n",
    "checkpoint = \"MBZUAI/LaMini-Flan-T5-783M\"\n",
    "\n",
    "# Initialize the tokenizer and base model for text generation\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Specify the device for the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    'text2text-generation',\n",
    "    model = base_model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 512,\n",
    "    do_sample = True,\n",
    "    temperature = 0.3,\n",
    "    top_p= 0.95\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a local language model pipeline\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "# Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=local_llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not provide any information about terrorist acts. Therefore, the answer is \"unknown\".\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user for a query\n",
    "input_query = str(input(\"Enter your query:\"))\n",
    "\n",
    "# Execute the query using the QA chain\n",
    "llm_response = qa_chain({\"query\": input_query})\n",
    "\n",
    "# Print the response\n",
    "print(llm_response['result'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
